{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture outline\n",
    "\n",
    "- Wave hello\n",
    "- **!! Turn on recording !!**\n",
    "- Announcements (5 min)\n",
    "- Cilantro dataset (5 min)\n",
    "- Decision trees (30 min)\n",
    "- Break (5 min)\n",
    "- True/False questions (15 min)\n",
    "- ML model parameters and hyperparameters (5 min)\n",
    "- Overfitting (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from plot_classifier import plot_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def display_tree(feature_names, tree):\n",
    "    \"\"\" For binary classification only \"\"\"\n",
    "    dot = export_graphviz(tree, out_file=None, feature_names=feature_names, class_names=tree.classes_,impurity=False)\n",
    "    # adapted from https://stackoverflow.com/questions/44821349/python-graphviz-remove-legend-on-nodes-of-decisiontreeclassifier\n",
    "    dot = re.sub('(\\\\\\\\nsamples = [0-9]+)(\\\\\\\\nvalue = \\[[0-9]+, [0-9]+\\])(\\\\\\\\nclass = [A-Za-z0-9]+)', '', dot)\n",
    "    dot = re.sub(     '(samples = [0-9]+)(\\\\\\\\nvalue = \\[[0-9]+, [0-9]+\\])\\\\\\\\n', '', dot)\n",
    "    return graphviz.Source(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements (5 min)\n",
    "\n",
    "- hw1 due tonight at 11:59pm\n",
    "- hw2 will be released tomorrow, due Monday 11:59pm\n",
    "  - See [here](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md#groups) for instructions on working with a partner.\n",
    "  - You are free to work alone or with a partner.\n",
    "- On the usual schedule, hw will be due Mondays and released Tuesdays\n",
    "- My evening office hour moved from Wed to Thu \n",
    "  - Note I have 30 min morning OH and 30 min evening OH.\n",
    "- Update on the plan for the final exam:\n",
    "  - We will **not** have a regular 2.5 hour exam in the regular way.\n",
    "  - There will be a take-home, with a mix of coding and conceptual questions.\n",
    "  - The time window will be 24-48 hours (exact time window TBD).\n",
    "  - Open book.\n",
    "- Update on the plan for the midterm:\n",
    "  - We'll do it on Canvas during class time on Oct 22.\n",
    "  - This will be the one time you'll need to operate in the middle of the night if you're in a far time zone (sorry).\n",
    "  - Probably open book.\n",
    "- Please monitor Piazza (especially pinned posts and instructor posts) for announcements.\n",
    "- Sorry for the setup difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cilantro dataset (5 min)\n",
    "\n",
    "Here's the dataset you generated last class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/330-students-cilantro.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- head(<int>) prints the first 5 rows by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"meat\", \"grade\", \"cilantro\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- df.columns is used to rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- describe() gets the statistic for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(df[\"meat\"], df[\"grade\"], c=df[\"cilantro\"]==\"Yes\", cmap=plt.cm.coolwarm);\n",
    "plt.xlabel(\"Meat consumption (% days)\");\n",
    "plt.ylabel(\"Expected grade (%)\");\n",
    "plt.legend(scatter.legend_elements()[0], [\"No\", \"Yes\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter.legend_elements()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find yourself on this plot?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cilantro\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"meat\", \"grade\"]]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"cilantro\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier(strategy=\"prior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DummyClassifier predicts the most common class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.fit(X, y)\n",
    "dc.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fit() takes the data and does the learning, in this case(DummyClassifier), find the most common class\n",
    "- score() shows how well the DummyClassifier do on our data\n",
    "- 0.72 here means that the DummyClassifier gives a 72% correct prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees (20 min)\n",
    "\n",
    "- Our first approach to supervised learning: **decision trees**.\n",
    "- Basic idea: ask a bunch of yes/no questions until you end up at a prediction.\n",
    "- E.g. for our cilantro dataset,\n",
    "  - If you eat meat <5% of the time, predict \"Yes\"\n",
    "  - Otherwise, if you eat meat >95% of the time, predict \"No\"\n",
    "  - Otherwise, if you expect to fail the course, predict \"No\"\n",
    "  - Otherwise, predict \"Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This \"series of questions\" approach can be drawn as a tree:\n",
    "\n",
    "```\n",
    "            Eats meat <5% of the time\n",
    "            /          \\\n",
    "           / True       \\  False\n",
    "          /              \\\n",
    "         Yes           Eats meat >95% of the time\n",
    "                        /      \\\n",
    "                  True /        \\ False\n",
    "                      /          \\ \n",
    "                    No         Expects to fail the course (<50%)\n",
    "                                 /           \\\n",
    "                                / True        \\ False\n",
    "                               /               \\\n",
    "                              No              Yes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decision tree algorithm automatically learns a tree like this, based on the data set!\n",
    "  - We won't go through **how** it does this - that's CPSC 340.\n",
    "  - But it's worth noting that it support two types of inputs:\n",
    "\n",
    "1. Categorical (e.g., Yes/No or more options)\n",
    "2. Numeric (a number)\n",
    "\n",
    "In the numeric case, the decision tree algorithm also picks the _threshold_ (e.g. 5%, 50%, etc.)\n",
    "\n",
    "In our case here, both features are numeric. (meat & grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a decision tree to our cilantro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we create a `DecisionTreeClassifier` object from scikit-learn.\n",
    "- We pass in parameters - these are called **hyperparameters** - in this case `max_depth=1` which means the tree can only have depth 1. (A question/tree could spread no more than `max_depth` times.)\n",
    "- Next we fit to the data using `.fit()`.\n",
    "- The semicolon is just cosmetic, otherwise some junk gets printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(df.columns[:-1], tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a totally useless decision tree that predicts \"Yes\" for any feature.\n",
    "- This happens sometimes. Let's roll with it for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier(X, y, tree1, ticks=True, vmin=0, vmax=1); # note to self: need to set vmin/vmax to to an issue with plot_classifier that always draws blue if all predictions are the same\n",
    "plt.xlabel(\"Meat consumption (% days)\");\n",
    "plt.ylabel(\"Expected grade (%)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The background colour shows our prediction. (\"Yes\" here)\n",
    "- We predict red (likes cilantro) for any features.\n",
    "- We can get an accuracy score using `.score()` from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is doing the same thing as `DummyClassifier` so we get the same score.\n",
    "- We can verify this using `.predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.predict([[50, 50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A 50% meat eater and 50% course grade is gonna predict Cilantro == 'Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.predict([[99,99]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For all the people in the class, or all the data in the data set, gonna predict 'Yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's make the tree deeper by increasing `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2 = DecisionTreeClassifier(max_depth=2)\n",
    "tree2.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(df.columns[:-1], tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- df.columns[:-1] means escape the last column, here means that only get 'meat' and 'grade' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier(X, y, tree2, ticks=True, show_data=True);\n",
    "plt.xlabel(\"Meat consumption (% days)\");\n",
    "plt.ylabel(\"Expected grade (%)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a moment to make sure we can correspond the tree diagram to this diagram - they are saying the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By the way, what does `.score()` do?\n",
    "- It calls `predict` and then compares the predictions to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tree2.predict(X) == y).sum()/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, equivalently,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tree2.predict(X) == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to `max_depth=None`, which lets it grow the tree as much as it wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=None)\n",
    "tree.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict([[90, 90]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(df.columns[:-1], tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier(X, y, tree, ticks=True);\n",
    "plt.xlabel(\"Meat consumption (% days)\");\n",
    "plt.ylabel(\"Expected grade (%)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason it's not getting 100% accuracy: instances of duplicated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's OK if you don't understand this line\n",
    "df.loc[df.duplicated(subset=df.columns[:-1], keep=False)].sort_values(by=df.columns.values.tolist()).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove the \"duplicates\" (cases where X is the same, not y) then we can get 100% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's OK if you don't understand this line\n",
    "df_nodup = df.sort_values(by=\"cilantro\").drop_duplicates(subset=df.columns[:-1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nodup = df_nodup.iloc[:,:2]\n",
    "y_nodup = df_nodup.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_nodup = DecisionTreeClassifier() # default is max_depth=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_nodup.fit(X_nodup, y_nodup);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_nodup.score(X_nodup, y_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier(X_nodup, y_nodup, tree_nodup, ticks=True);\n",
    "plt.xlabel(\"Meat consumption (% days)\");\n",
    "plt.ylabel(\"Expected grade (%)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: one would not actually remove the duplicates in a real scenario. This is just for illustration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions (15 min)\n",
    "\n",
    "For each of the following, answer with `fit` or `predict`:\n",
    "\n",
    "1. At least for decision trees, this is where most of the hard work is done. `fit`\n",
    "2. Only takes `X` as an argument. `predict`\n",
    "3. In scikit-learn, we can ignore its output. (kind of a void function, we don't need to grab and store the result) `fit` ('predict()' has a return value)\n",
    "4. Is called first (before the other one). `fit`\n",
    "\n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ML model parameters and hyperparameters (5 mins)\n",
    "\n",
    "- When you call `fit`, a bunch of values get set, like the split variables and split thresholds. \n",
    "- These are called **parameters**.\n",
    "- But even before calling `fit` on a specific data set, we can set some \"knobs\" that control the learning, e.g. `max_depth`.\n",
    "- These are called **hyperparameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, hyperparameters are set in the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3) \n",
    "tree.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `max_depth` is a hyperparameter. There are many, many more! See [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize:\n",
    "\n",
    "- **parameters** are automatically learned by the algorithm during training (`fit`)\n",
    "- **hyperparameters** are specified by the human, before `fit` (decided in advanced, a pre-decided value), based on:\n",
    "    - expert knowledge\n",
    "    - heuristics, or \n",
    "    - systematic/automated optimization (more on that later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting (15 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important question: how does accuracy change vs. max_depth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# it would be good to understand this code, but not that urgent\n",
    "# I am using a list comprehension but you might find it easier to understand with a `for` loop - post on Piazza for more info\n",
    "max_depths = np.arange(1, 18)\n",
    "scores = [DecisionTreeClassifier(max_depth=max_depth).fit(X_nodup, y_nodup).score(X_nodup, y_nodup) for max_depth in max_depths]\n",
    "plt.plot(max_depths, scores);\n",
    "plt.xlabel(\"max depth\");\n",
    "plt.ylabel(\"accuracy score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why not just use a very deep decision tree for every supervised learning problem and get super high accuracy?\n",
    "- Well, the goal of supervised learning is to predict unseen/new data...\n",
    "  - The above decision tree has 100% accuracy on the training data **where we already know the answer**.\n",
    "  - It perfectly labels the data we used to make the tree...\n",
    "  - But we want to know how our model performs on data not used in training.\n",
    "  - We will split our original dataset into two parts, one for \"training\" and one for \"testing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_nodup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the data set into train examples and test examples, 75% and 25% of the data set respectively by defdalult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(df_train[\"meat\"], df_train[\"grade\"], c=df_train[\"cilantro\"]==\"Yes\", cmap=plt.cm.coolwarm);\n",
    "plt.xlabel(\"Meat consumption (% days)\");\n",
    "plt.ylabel(\"Expected grade (%)\");\n",
    "plt.legend(scatter.legend_elements()[0], [\"No\", \"Yes\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(df_test[\"meat\"], df_test[\"grade\"], c=df_test[\"cilantro\"]==\"Yes\", cmap=plt.cm.coolwarm);\n",
    "plt.xlabel(\"Meat consumption (% days)\");\n",
    "plt.ylabel(\"Expected grade (%)\");\n",
    "plt.xlim((0,100));\n",
    "plt.ylim((0,100));\n",
    "plt.legend(scatter.legend_elements()[0], [\"No\", \"Yes\"]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
